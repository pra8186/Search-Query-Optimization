# -*- coding: utf-8 -*-
"""Query_Optimization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pi3Qh0pYSM_mNM7tEIumuHt7w45H9oXF
"""

import sys

f = open('/content/Medical_Abstracts.txt', "r")
ctr=0
d={1:[]} # This dictionary will hold the doc Id and the abstract data from the document as the key-value pairs
flag=True
ctr=1
for i in f.readlines():
  j=i.strip("\n")
  j=j.strip(" ")
  j=j.replace('/',' ')
  if j[0:2]==".I": # Parsing through the document and looking for the end of line checkpoints
    ctr+=1 # In case the end of the doc or abstract is reached then the ctr(For keeping the record of the Doc Ids) value will be incremented and the data will be add to the next counter value.
    d[ctr]=[]
    flag=False
  elif flag:
    d[1].append(j)
  else:
    d[ctr].append(j)
for i in d.keys():
  d[i]=d[i][d[i].index('.W')+1:] # Removing ".W"
  d[i]=[" ".join(d[i])]

print("The total number of the documents present in the file are",len(d.keys()))

# Printing the data contained under the 1 doc Id in the dictionary d
print(" ".join(d[1]))
print(d)

# Doing the Data-preprocessing for removing stop words
import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
corpus=[] # Corpus is the set of unoque words that are used in the document
for i in d.keys():
  stop_words = set(stopwords.words('english'))
  word_tokens = word_tokenize(' '.join(d[i]))
  temp=[]
  for w in word_tokens:
    if w.lower() not in stop_words:
      if w not in ['.','/',',',' ',"=","+","-","%","(",")","'","*",'']:
        w=w.strip("'")
        w=w.strip("%")
        w=w.strip('/')
        w=w.strip('.')
        w=w.strip(' ')
        w=w.strip('+')
        w=w.strip("-")
        w=w.strip(")")
        w=w.strip("(")
        if "/" in w:
          w=w.replace("/"," ")
        w=lemmatizer.lemmatize(w)
        temp.append(w)
  d[i]=temp
  for i in temp:
    if i not in corpus:
      corpus.append(i)

# The number of the unique words in the vocublary
print("The number of unique words used in the document are ",len(corpus))

print(d)

"""### Relevance Ratio for Relevant Documents"""

import random

li= {}
st =set()

while len(st) < 100:
       st.add(random.randint(1 , 492))
st =  list(st)

st

def relvejudge1(docID , q , DocID):
          if(DocID  == docID):
            return  True
          else:
             q  = query_clean(q) ;
             words = d[docID]
             count  = 0 ;

             for i in  q:
                 for j in words:
                    if(i == j):
                      count+=1 ;

             ratio =  count/len(q)  ;
             return ratio > 0.07 ;

def relvejudge2(docID , q):
             q  = query_clean(q) ;
             words = d[docID]
             count  = 0 ;

             for i in  q:
                 for j in words:
                    if(i == j):
                      count+=1

             ratio =  count/len(q)
             return ratio > 0.5;

def relvejudge3(docID , q):
             q  = query_clean(q)
             words = d[docID]
             count  = 0

             for i in  q:
                 for j in words:
                    if(i == j):
                      count+=1

             ratio =  count/len(q)
             return ratio >= 0.03

queries = {
"What produces acetoacetate?" : 9 ,
"Which mineral manifests a lipotropic effect after toxication of the rat by tetrachlorocarbon?" : 50  ,
"How many nucleated cells were found on an average in the bone marrow of a normal guinea-pig?":  61 ,
"How does cytoplasmic inclusions within macrophaegs of human tissues appears?": 78,
"What happened to the mitotic activity of the lens epithelium at high altitudes?" :  79,
"What is the time delay in the rise of CO2 tension in jugular venous blood and cisternal cerebrospinal fluid following CO administration?":  80,
"What are the time constants for the fast and slow components of the rise in CO2 tension in arterial blood, jugular venous blood, and cisternal cerebrospinal fluid?": 80,
"What is the conclusion about the use of cesium-131 in studying cardiac morphology?": 82,
"What were the findings of the study on maternal anxiety during pregnancy and its effect on mother and child adjustment eight months following childbirth?": 98,
"What is the pilot program designed to measure and develop rehabilitative material for, and who collected data and observed patients in the program?": 110,
"What were the associated anomalies associated with ventricular septal defect?": 112,
"What is the combination of ventricular septal defect and aortic insufficiency?": 115,
"What was observed in the bone marrow of normal and irradiated rats?": 123,
"What is the field method for sampling nickel carbonyl?": 128,
"What is the method of choice for heavy metal analysis in air samples?": 129,
"Which metals were found when analysing inorganic constituents of human teeth?": 130,
"How do metal ions affect TMV-RNA?": 132,
"What were the effects of high levels of antioxidants on chicks with a combined deficiency of selenium and vitamin E?": 134,
"When is mitral valvulotomy indicated in pregnancy? ": 139,
"What enzyme is responsible for the repair of partially single-stranded DNA templates?": 143
}

#queries=["What produces acetoacetate?",
#"Which mineral manifests a lipotropic effect after toxication of the rat by tetrachlorocarbon?",
#"How many nucleated cells were found on an average in the bone marrow of a normal guinea-pig?",
#"How does cytoplasmic inclusions within macrophaegs of human tissues appears?",
#"What happened to the mitotic activity of the lens epithelium at high altitudes?",
#"What is the time delay in the rise of CO2 tension in jugular venous blood and cisternal cerebrospinal fluid following CO administration?",
#"What are the time constants for the fast and slow components of the rise in CO2 tension in arterial blood, jugular venous blood, and cisternal cerebrospinal fluid?",
#"What is the conclusion about the use of cesium-131 in studying cardiac morphology?",
#"What were the findings of the study on maternal anxiety during pregnancy and its effect on mother and child adjustment eight months following childbirth?",
#"What is the pilot program designed to measure and develop rehabilitative material for, and who collected data and observed patients in the program?",
#"What were the associated anomalies associated with ventricular septal defect?",
#"What is the combination of ventricular septal defect and aortic insufficiency?",
#"What was observed in the bone marrow of normal and irradiated rats?",
#"What is the field method for sampling nickel carbonyl?",
#"What is the method of choice for heavy metal analysis in air samples?",
#"Which metals were found when analysing inorganic constituents of human teeth?",
#"How do metal ions affect TMV-RNA?",
#"What were the effects of high levels of antioxidants on chicks with a combined deficiency of selenium and vitamin E?",
#"When is mitral valvulotomy indicated in pregnancy? ",
#"What enzyme is responsible for the repair of partially single-stranded DNA templates?"]

score = []
for j in queries.keys():
    temp1 = []
    for k in st:
        temp  =  queries[j]
        sc1 =  relvejudge1(k , j , temp) ;
        sc2 =  relvejudge2(k , j ) ;
        sc3 =  relvejudge3(k , j ) ;
        temp1.append([sc1 , sc2 , sc3])
    score.append(temp1)

def RelvanceOverall(score):
      arr = [0 , 0 , 0]
      # scores is [  (queries[doc1[score1 , score2 , score3] , doc2 , doc3]
      scores_1 = []
      print(len(score))
      for i in score:
        #  print(i)

         for j in range (0  ,len(i)) :
            for k in range(0 , len(i[j])):
               arr[k]+=i[j][k] ;
        #  print(arr)
         for i1 in range(0 , len(arr)):
             if( i1 ==  0):
                 arr[i1] = arr[i1] >= 29
             if( i1 ==  1):
                 arr[i1] = arr[i1]  >= 2
             if( i1 ==  2):
                 arr[i1] = arr[i1]  >= 22
        #  print(arr)
         scores_1.append(tuple(arr)) ;
        #  print(scores_1)
      return scores_1

q_label_revelant = []
q_label_non_revelant = []
print(len(score) , len(score[1]))
for i in score:
    temp =0  ;
    app = []
    app_ = []
    ind =  0 ;
    for j in i :
       for k in j :
          temp+=k ;
       if(temp >  1) :
          app.append(st[ind]) ;
       else :
          app_.append(st[ind]) ;
       ind+=1 ;
    q_label_revelant.append(app) ;
    q_label_non_revelant.append(app_) ;
    # q_label.append(app) ;

print(len(q_label_revelant) , len(q_label_non_revelant))
print(q_label_non_revelant)
print(q_label_revelant)

scores  = RelvanceOverall(score)
print(scores)

def kappaCaculator(scores , j1 , j2):
       same_ = 0 ;
      #  diff_ = 0 ;
       relevant  = 0 ;
       non_relevant  = 0 ;
       for i in scores:
           same_ += 100*(i[j1]==i[j2] ) ;
           relevant+=100* (i[j1] == 1)+100*(i[j2]==1)
           non_relevant+=100*(i[j1]==0)+100*(i[j2] == 0)

       p_a = same_/2000 ;
      #  print(p_a)
      #  print(relevant)
      #  print(non_relevant)
       p_e = (relevant/4000)**2 + (non_relevant/4000)**2
      #  print(p_e)
       return (p_a -  p_e)/(1- p_e)

k1_2 =  kappaCaculator(scores , 0 , 1) ;
k1_3 =  kappaCaculator(scores ,0 , 2) ;
k2_3=  kappaCaculator(scores , 1 , 2) ;
print("avg kappa value: " ,  (k1_2+k1_3+k2_3)/3)

"""### Q1_1_Part B

#### Creating the Inverted Index
"""

def Indexes(id,word):
  inverted_index=[] # This dictionary will contain the word as key and the [count, filename]
  for j in id: # Checking the availability of the word in all the documnets for creating the posting list
    if word in d[j]:
      inverted_index.append(j)
  return inverted_index

"""#### tf-idf vector Model"""

# Function for giving the tf-idf scores for a word
def tf_idf(doc,word):
  max_cout=0
  for i in d[doc]:
    max_cout=max(max_cout,d[doc].count(i)) # Finding out the max occurence of a word in a particular document
  score=0.5+((0.5*d[doc].count(word))/max_cout) # Formula from the slides for calculating the tf-idf score of a word in a document
  return score

"""#### Query Preprocessing"""

from nltk.tokenize import word_tokenize
# This function will padd the query words and tokenize the given query sentence to create the wordlist
def query_clean(q):
  stop_words = set(stopwords.words('english'))
  word_tokens = word_tokenize(q)
  wordlist=[]
  for i in word_tokens:
    i=i.strip("?")
    i=i.strip(".")
    if i.lower() not in stop_words:
      i=lemmatizer.lemmatize(i)
      wordlist.append(i)
  return wordlist # This will return the tokenized form of the query sentence

"""##### Calling the Tf-Idf vector model to give the list of the relevant documents"""

import numpy as np
import random
def relevant_doc(wordlist,corpora):
  top_doc={} # This array will contain the doc-Id along with its tf-idf score for that query
  union=[]
  #rand_Id=random.sample(d.keys(), 10)
  for j in wordlist:
    doclist=Indexes(corpora,j)
    if len(doclist)!=0:
      for k in doclist: # parse through the doclist and check whether that doc Id is alredy present in the union set or not
        if k not in union: # add the doc Id to the union set in case it is not already present
          union.append(k)
  if len(union)==0:
    return [0]
  for i in union: # Parsing through the union set
    score=0
    for j in wordlist: # Parse through all the query words for a particular doc to calculate the the tf-idf score of that particular document for that query
      score+=tf_idf(i,j) # Add all the scores of each wprd to get the score of that document for the query
    top_doc[i]=score
  ans=[]
  #print(top_doc)
  val=list(top_doc.values())
  val.sort(reverse=True)
  doc_count=0
  num=0
  while num<len(val) and doc_count<10:
    score=val[num]
    for j in top_doc.keys():
      if top_doc[j]==score:
        ans.append(j)
        break
    num+=1
    doc_count+=1
  return ans

"""#### MAP and Avg. Precision metric"""

def sum(num_list):
  s=0
  for i in num_list:
    s+=i
  return s

def average_precision(relevant_indices, retrieved_indices):
  precision =[]
  relevant = 0
  count=0
  for i in retrieved_indices:
    if i in relevant_indices:
      relevant += 1
      precision.append(relevant / (count+1))
    count+=1
  if len(precision)==0 or len(retrieved_indices)==0 or len(relevant_indices)==0:
    return 0,0,0
  return (sum(precision)/len(precision)),(relevant/len(retrieved_indices)),(relevant/len(relevant_indices))

ctr=0
for i in queries.keys():
  cleaned=query_clean(i)
  map=[]
  precision=[]
  recall=[]
  rel_docs=q_label_revelant[ctr]
  for k in range(10):
    rand_Id=random.sample(d.keys(), 100)
    ans=relevant_doc(cleaned,rand_Id)
    avg_pre,pre,re=average_precision(rel_docs,ans)
    map.append(avg_pre)
    precision.append(pre)
    recall.append(re)
  mean_map=sum(map)/len(map)
  mean_precision=sum(precision)/len(precision)
  mean_recall=sum(recall)/len(recall)
  print("MAP value:",mean_map," Average Precision:",mean_precision," Average Recall:",mean_recall)
  #print(mean_map)
  ctr+=1

"""### Q1 Part2

#### Document Vectors
"""

def one_hot_vector(s):
  temp=[0]*len(corpus)
  ctr=0
  for j in corpus:
    if j in s:
      temp[ctr]=1
    ctr+=1
  return temp

d_vec={}
for i in range(1,493):
  d_vec[i]=one_hot_vector(d[i])

"""#### Psuedo-Relevanve feedback"""

def mul(vector,num):
  dummy=[]
  for i in vector:
    dummy.append(num*i)
  return dummy

ctr=0
for i in queries.keys():
  cleaned=query_clean(i)
  rand_Id=random.sample(d.keys(), 100)
  ans=relevant_doc(cleaned,rand_Id)
  q_vector=one_hot_vector(cleaned)
  alpha=0.2
  map=[]
  while alpha<1:
    summation=[0]*len(corpus)
    if ans[0]!=0:
      for k in ans:
        summation=summation+d_vec[k]
    q_vector=mul(q_vector,alpha)+mul(summation,(1-alpha)/len(ans))
    cleaned=[]
    for j in range(len(corpus)):
      if q_vector[j]<0:
        q_vector[j]=0
      if q_vector[j]>0:
        cleaned.append(corpus[j])
    map_temp=[]
    rel_docs=q_label_revelant[ctr]
    for k in range(10):
      rand_Id=random.sample(d.keys(), 100)
      ans=relevant_doc(cleaned,rand_Id)
      avg_pre,pre,re=average_precision(rel_docs,ans)
      map_temp.append(avg_pre)
    dummy_map=0
    for add in map_temp:
      dummy_map+=add
    map.append([dummy_map/len(map_temp),alpha])
    alpha+=0.2
  map.sort(reverse=True)
  print("List of corresponding MAP value and alpha value",map)
  print(f"best alpha value for query {ctr+1} is",map[0][1],"\n")
  ctr+=1

"""## The 20-queries are:
What produces acetoacetate? 9    <br>
Which mineral manifests a lipotropic effect after toxication of the rat by tetrachlorocarbon? 50   <br>
How many nucleated cells were found on an average in the bone marrow of a normal guinea-pig? 61  <br>
How does cytoplasmic inclusions within macrophaegs of human tissues appears? 78  <br>
What happened to the mitotic activity of the lens epithelium at high altitudes? 79  <br>
What is the time delay in the rise of CO2 tension in jugular venous blood and cisternal cerebrospinal fluid following CO administration? 80  <br>
What are the time constants for the fast and slow components of the rise in CO2 tension in arterial blood, jugular venous blood, and cisternal cerebrospinal fluid? 80  <br>
What is the conclusion about the use of cesium-131 in studying cardiac morphology? 82  <br>
What were the findings of the study on maternal anxiety during pregnancy and its effect on mother and child adjustment eight months following childbirth? 98  <br>
What is the pilot program designed to measure and develop rehabilitative material for, and who collected data and observed patients in the program? 110  <br>
What were the associated anomalies associated with ventricular septal defect? 112  <br>
What is the combination of ventricular septal defect and aortic insufficiency? 115  <br>
What was observed in the bone marrow of normal and irradiated rats? 123  <br>
What is the field method for sampling nickel carbonyl? 128  <br>
What is the method of choice for heavy metal analysis in air samples? 129  <br>
Which metals were found when analysing inorganic constituents of human teeth? 130  <br>
How do metal ions affect TMV-RNA? 132  <br>
What were the effects of high levels of antioxidants on chicks with a combined deficiency of selenium and vitamin E? 134  <br>
When is mitral valvulotomy indicated in pregnancy? 139  <br>
What enzyme is responsible for the repair of partially single-stranded DNA templates? 143  <br>

#### With Simple tf-idf model
"""

ctr=0
from nltk.corpus import wordnet
for i in queries.keys():
  cleaned=query_clean(i)
  temp=[]
  for j in cleaned:
    syns = wordnet.synsets(j)
    ctr=0
    for k in syns:
      if ctr>1:
        break
      temp.append(k.lemmas()[0].name())
      ctr+=1
  cleaned+=temp
  map=[]
  precision=[]
  recall=[]
  rel_docs=q_label_revelant[ctr]
  for k in range(10):
    rand_Id=random.sample(d.keys(), 10)
    ans=relevant_doc(cleaned,rand_Id)
    avg_pre,pre,re=average_precision(rel_docs,ans)
    map.append(avg_pre)
    precision.append(pre)
    recall.append(re)
  mean_map=sum(map)/len(map)
  mean_precision=sum(precision)/len(precision)
  mean_recall=sum(recall)/len(recall)
  print("MAP value:",mean_map,"Average Precision value:",mean_precision,"Average Recall Value",mean_recall)
  ctr+=1

"""##### By Adding the Synnonms the model is giving better results but due to random sampling in some cases the results are not as per requirement by in rest of the cases we are able to get better results in comparison to the earlier model

#### With pseudo relevance feedback
"""

q_ctr=0
for i in queries:
  cleaned=query_clean(i)
  temp=[]
  for j in cleaned:
    syns = wordnet.synsets(j)
    ctr=0
    for k in syns:
      if ctr>1:
        break
      temp.append(k.lemmas()[0].name())
      ctr+=1
  cleaned+=temp
  rand_Id=random.sample(d.keys(), 100)
  ans=relevant_doc(cleaned,rand_Id)
  q_vector=one_hot_vector(cleaned)
  alpha=0.2
  map=[]
  while alpha<1:
    summation=[0]*len(corpus)
    if ans[0]!=0:
      for k in ans:
        summation=summation+d_vec[k]
    q_vector=mul(q_vector,alpha)+mul(summation,(1-alpha)/len(ans))
    cleaned=[]
    for j in range(len(corpus)):
      if q_vector[j]<0:
        q_vector[j]=0
      if q_vector[j]>0:
        cleaned.append(corpus[j])
    map_temp=[]
    rel_docs=q_label_revelant[q_ctr]
    #print(rel_docs," ")
    for k in range(10):
      rand_Id=random.sample(d.keys(), 100)
      ans=relevant_doc(cleaned,rand_Id)
      #print(ans,"ans")
      avg_pre,pre,re=average_precision(rel_docs,ans)
      #print(avg_pre)
      map_temp.append(avg_pre)
    dummy_map=sum(map_temp)/len(map_temp)
    map.append([dummy_map,alpha])
    alpha+=0.2
  map.sort(reverse=True)
  print("the list of the MAP and alpha values is",map)
  print(f"best alpha value for query {q_ctr+1} is",map[0][1],"\n")
  q_ctr+=1

"""### K-Means Clustering


"""

def jaccard(list1, list2):
    intersection = len(list(set(list1).intersection(list2)))
    union = (len(list1) + len(list2)) - intersection
    return float(intersection) / union

import numpy as nm
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances
distance = []
for i in range(1,493):
  temp=[]
  for j in range(1,493):
    temp.append(1-jaccard(d[i],d[j]))
  distance.append(temp)
#pairwise_distances(X, metric='jaccard')
K_range= range(1, 23)
RSS = []
for K in K_range:
    kmeans = KMeans(n_clusters=K, random_state=1)
    kmeans.fit(nm.array(distance))
    RSS.append(kmeans.inertia_)
plt.plot(K_range, RSS)
plt.xlabel('Values of K')
plt.ylabel('RSS value ')
plt.title('RSS vs K')
plt.show()

"""##### Optimal number of clusters can not be found in this case because the sjape of the curve is not elbow shaped."""

import random
from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_rand_score
from sklearn.cluster import KMeans, AgglomerativeClustering

print(nm.array(distance).shape)

"""##### Let the number of optimal clusters be 5"""

import numpy as np
from sklearn.metrics import pairwise_distances
from sklearn.cluster import AgglomerativeClustering, KMeans
from sklearn.metrics.cluster import normalized_mutual_info_score
def calculate_purity(labels, true_labels):
    majority_sum = 0
    #print(labels)
    for k in np.unique(labels):
        mask = labels == k
        #print(k,mask)
        true_labels_k = true_labels[mask]
        #print(true_labels_k)
        majority_label = np.bincount(true_labels_k).argmax()
        #print(majority_label,"m")
        majority_sum += np.sum(true_labels_k == majority_label)
        #print(majority_sum)
    return majority_sum / len(labels)

# Apply k-means clustering with k=2 to obtain true labels
kmeans = KMeans(n_clusters=5, random_state=42)
true_labels = kmeans.fit_predict(nm.array(distance))
#print(true_labels)
# Apply hierarchical clustering with single linkage
k = 5
D=nm.array(distance)
hc_single = AgglomerativeClustering(n_clusters=k, affinity='precomputed', linkage='single')
clusters_single = hc_single.fit_predict(nm.array(distance))
#print(clusters_single)

# Calculate purity and NMI values for single linkage
purity_single = calculate_purity(clusters_single, true_labels)
nmi_single = normalized_mutual_info_score(true_labels, clusters_single)

# Apply hierarchical clustering with complete linkage
hc_complete = AgglomerativeClustering(n_clusters=k, affinity='precomputed', linkage='complete')
clusters_complete = hc_complete.fit_predict(D)
#print(clusters_complete)

# Calculate purity and NMI values for complete linkage
purity_complete = calculate_purity(clusters_complete, true_labels)
nmi_complete = normalized_mutual_info_score(true_labels, clusters_complete)

# Apply hierarchical clustering with centroid-based linkage
hc_centroid = AgglomerativeClustering(n_clusters=k, affinity='precomputed', linkage='average')
clusters_centroid = hc_centroid.fit_predict(D)

# Calculate purity and NMI values for centroid-based linkage
purity_centroid = calculate_purity(clusters_centroid, true_labels)
nmi_centroid = normalized_mutual_info_score(true_labels, clusters_centroid)
 # Print purity and NMI values for all linkages
print("Purity (single linkage):", purity_single)
print("NMI (single linkage):", nmi_single)
print("Purity (complete linkage):", purity_complete)
print("NMI (complete linkage):", nmi_complete)
print("Purity (centroid-based linkage):", purity_centroid)
print("NMI (centroid-based linkage):", nmi_centroid)

